{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb971367a0de4474bb3e909fe978acb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71ae27e6ffbc4de781744e052aade6dc",
              "IPY_MODEL_9b647755616d40dc8c2c185cc72cf942",
              "IPY_MODEL_01103a090116433ca58eef7499a2dd65"
            ],
            "layout": "IPY_MODEL_75e7d4bb665e40cebc023a9c22349992"
          }
        },
        "71ae27e6ffbc4de781744e052aade6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a466be590304b4eb361e142ba02cde6",
            "placeholder": "​",
            "style": "IPY_MODEL_848e9b12d1b7448da878b6e7e3dd787b",
            "value": "Map: 100%"
          }
        },
        "9b647755616d40dc8c2c185cc72cf942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26ffcf0a24174eba81759bc224fc6ee6",
            "max": 2869,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ef9f031fd7c44af903d4854bc00a170",
            "value": 2869
          }
        },
        "01103a090116433ca58eef7499a2dd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_515646b583104163a5ff90edd27d4ea7",
            "placeholder": "​",
            "style": "IPY_MODEL_b0aa5ac6da524b0f8817fb55d715db52",
            "value": " 2869/2869 [00:01&lt;00:00, 1712.64 examples/s]"
          }
        },
        "75e7d4bb665e40cebc023a9c22349992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a466be590304b4eb361e142ba02cde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848e9b12d1b7448da878b6e7e3dd787b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26ffcf0a24174eba81759bc224fc6ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef9f031fd7c44af903d4854bc00a170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "515646b583104163a5ff90edd27d4ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0aa5ac6da524b0f8817fb55d715db52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbcbb09a91684c8a8d18d07f4ee3839e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f98a1644b6d4e14a018c1a260fe0d9b",
              "IPY_MODEL_339062c31a5549b18358f0f85972cf72",
              "IPY_MODEL_1fa5de09afab4549b7a30ee572814dd9"
            ],
            "layout": "IPY_MODEL_a221e2ba28844afea53663dd148e21c2"
          }
        },
        "9f98a1644b6d4e14a018c1a260fe0d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706228efe6a54295ad761774b855ea2c",
            "placeholder": "​",
            "style": "IPY_MODEL_1b8e7bd936174689af806ce99868f0a8",
            "value": "Map: 100%"
          }
        },
        "339062c31a5549b18358f0f85972cf72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3463a5183c9d4f5d8e84c2cae7cf3c8f",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b25bee5320e94221891b887caafe47cc",
            "value": 718
          }
        },
        "1fa5de09afab4549b7a30ee572814dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d706dcdce24d46c6a8ac70def6d31d0a",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c6822aa0ff4ea38962765be9d69660",
            "value": " 718/718 [00:00&lt;00:00, 1692.48 examples/s]"
          }
        },
        "a221e2ba28844afea53663dd148e21c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706228efe6a54295ad761774b855ea2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8e7bd936174689af806ce99868f0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3463a5183c9d4f5d8e84c2cae7cf3c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b25bee5320e94221891b887caafe47cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d706dcdce24d46c6a8ac70def6d31d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c6822aa0ff4ea38962765be9d69660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xACQ_Moi9zdY"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch, numpy as np, random, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "9YmjWxG7-Dev"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45BCXFZt-H13",
        "outputId": "3ae4e785-b5dd-43d3-c1f7-f3e2fa5589cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e5b91f64730>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU (slower).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbj0-g_L-Jmn",
        "outputId": "23305368-a302-41b8-bee9-d5f9bf6bcc41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "dtbXOVO8-Jz_",
        "outputId": "b17c314d-9533-4773-8006-2f8ecce0a658"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b59b3f4-872c-486a-a2c3-585764e32b9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b59b3f4-872c-486a-a2c3-585764e32b9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amazon_eco-friendly_products.csv to amazon_eco-friendly_products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"amazon_eco-friendly_products.csv\")\n",
        "print(\"Rows:\", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "UYFInCjb-J9f",
        "outputId": "a5ee40e5-2aef-462d-f8f4-9b03a9a6169b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 3587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id                                              title  \\\n",
              "0  B0CWH366KJ  Agfabric Natural Jute Erosion Control, 16yard(...   \n",
              "1  B086L692VC  SAFAVIEH Braided Collection 4' Round Light Blu...   \n",
              "2  B01J6JELTG  Eyeseals 4.0 Sleep Mask – Clear – Moisturizing...   \n",
              "3  B07HQSKK36  Lucky Monet 25/50/100PCS Burlap Gift Bags Wedd...   \n",
              "4  B0C3Y8WJDR  St. Boniface Bag Company | Burlap Bags - Size:...   \n",
              "\n",
              "                  name              category                       material  \\\n",
              "0  Weed Barrier Fabric  Patio, Lawn & Garden                            NaN   \n",
              "1            Area Rugs        Home & Kitchen  50%jute, 25% Wool, 25% Cotton   \n",
              "2       Sleeping Masks    Health & Household                        Plastic   \n",
              "3            Gift Bags    Health & Household                         Burlap   \n",
              "4            Grow Bags  Patio, Lawn & Garden                      5.0 Count   \n",
              "\n",
              "         brand   price  rating  reviewsCount  \\\n",
              "0     Agfabric   $87.3     NaN           NaN   \n",
              "1     Safavieh  $40.63     4.2          59.0   \n",
              "2       EYEECO  $65.95     3.7        1075.0   \n",
              "3  Lucky Monet  $29.99     4.6        2492.0   \n",
              "4      Generic  $29.99     4.4          11.0   \n",
              "\n",
              "                                         description  \\\n",
              "0  Protect your yard and garden with our biodegra...   \n",
              "1  Country style is perfect for a casual cottage ...   \n",
              "2  Locks moisture in: Eyeseals 4.0 eye mask for d...   \n",
              "3  ❤ Premium Burlap Material❤ These small burlap ...   \n",
              "4  100% Burlap > 100% BIODEGRADABLE AND ECO FRIEN...   \n",
              "\n",
              "                                    url  \\\n",
              "0  https://www.amazon.com/dp/B0CWH366KJ   \n",
              "1  https://www.amazon.com/dp/B086L692VC   \n",
              "2  https://www.amazon.com/dp/B01J6JELTG   \n",
              "3  https://www.amazon.com/dp/B07HQSKK36   \n",
              "4  https://www.amazon.com/dp/B0C3Y8WJDR   \n",
              "\n",
              "                                             img_url inStock  \\\n",
              "0  https://m.media-amazon.com/images/I/71t3FD5KjH...    True   \n",
              "1  https://m.media-amazon.com/images/I/A1Q73Cheh2...    True   \n",
              "2  https://m.media-amazon.com/images/I/61Uz393xlp...    True   \n",
              "3  https://m.media-amazon.com/images/I/71DrHIU1aM...    True   \n",
              "4  https://m.media-amazon.com/images/I/81q3el899U...    True   \n",
              "\n",
              "                          inStockText  \n",
              "0  Only 5 left in stock - order soon.  \n",
              "1  Only 3 left in stock - order soon.  \n",
              "2                                 NaN  \n",
              "3                  In Stock  In Stock  \n",
              "4                            In Stock  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc87e156-461a-4c0e-ba3e-41510af24516\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>name</th>\n",
              "      <th>category</th>\n",
              "      <th>material</th>\n",
              "      <th>brand</th>\n",
              "      <th>price</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviewsCount</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "      <th>img_url</th>\n",
              "      <th>inStock</th>\n",
              "      <th>inStockText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B0CWH366KJ</td>\n",
              "      <td>Agfabric Natural Jute Erosion Control, 16yard(...</td>\n",
              "      <td>Weed Barrier Fabric</td>\n",
              "      <td>Patio, Lawn &amp; Garden</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Agfabric</td>\n",
              "      <td>$87.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Protect your yard and garden with our biodegra...</td>\n",
              "      <td>https://www.amazon.com/dp/B0CWH366KJ</td>\n",
              "      <td>https://m.media-amazon.com/images/I/71t3FD5KjH...</td>\n",
              "      <td>True</td>\n",
              "      <td>Only 5 left in stock - order soon.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B086L692VC</td>\n",
              "      <td>SAFAVIEH Braided Collection 4' Round Light Blu...</td>\n",
              "      <td>Area Rugs</td>\n",
              "      <td>Home &amp; Kitchen</td>\n",
              "      <td>50%jute, 25% Wool, 25% Cotton</td>\n",
              "      <td>Safavieh</td>\n",
              "      <td>$40.63</td>\n",
              "      <td>4.2</td>\n",
              "      <td>59.0</td>\n",
              "      <td>Country style is perfect for a casual cottage ...</td>\n",
              "      <td>https://www.amazon.com/dp/B086L692VC</td>\n",
              "      <td>https://m.media-amazon.com/images/I/A1Q73Cheh2...</td>\n",
              "      <td>True</td>\n",
              "      <td>Only 3 left in stock - order soon.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B01J6JELTG</td>\n",
              "      <td>Eyeseals 4.0 Sleep Mask – Clear – Moisturizing...</td>\n",
              "      <td>Sleeping Masks</td>\n",
              "      <td>Health &amp; Household</td>\n",
              "      <td>Plastic</td>\n",
              "      <td>EYEECO</td>\n",
              "      <td>$65.95</td>\n",
              "      <td>3.7</td>\n",
              "      <td>1075.0</td>\n",
              "      <td>Locks moisture in: Eyeseals 4.0 eye mask for d...</td>\n",
              "      <td>https://www.amazon.com/dp/B01J6JELTG</td>\n",
              "      <td>https://m.media-amazon.com/images/I/61Uz393xlp...</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B07HQSKK36</td>\n",
              "      <td>Lucky Monet 25/50/100PCS Burlap Gift Bags Wedd...</td>\n",
              "      <td>Gift Bags</td>\n",
              "      <td>Health &amp; Household</td>\n",
              "      <td>Burlap</td>\n",
              "      <td>Lucky Monet</td>\n",
              "      <td>$29.99</td>\n",
              "      <td>4.6</td>\n",
              "      <td>2492.0</td>\n",
              "      <td>❤ Premium Burlap Material❤ These small burlap ...</td>\n",
              "      <td>https://www.amazon.com/dp/B07HQSKK36</td>\n",
              "      <td>https://m.media-amazon.com/images/I/71DrHIU1aM...</td>\n",
              "      <td>True</td>\n",
              "      <td>In Stock  In Stock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B0C3Y8WJDR</td>\n",
              "      <td>St. Boniface Bag Company | Burlap Bags - Size:...</td>\n",
              "      <td>Grow Bags</td>\n",
              "      <td>Patio, Lawn &amp; Garden</td>\n",
              "      <td>5.0 Count</td>\n",
              "      <td>Generic</td>\n",
              "      <td>$29.99</td>\n",
              "      <td>4.4</td>\n",
              "      <td>11.0</td>\n",
              "      <td>100% Burlap &gt; 100% BIODEGRADABLE AND ECO FRIEN...</td>\n",
              "      <td>https://www.amazon.com/dp/B0C3Y8WJDR</td>\n",
              "      <td>https://m.media-amazon.com/images/I/81q3el899U...</td>\n",
              "      <td>True</td>\n",
              "      <td>In Stock</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc87e156-461a-4c0e-ba3e-41510af24516')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc87e156-461a-4c0e-ba3e-41510af24516 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc87e156-461a-4c0e-ba3e-41510af24516');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-72b8fec8-298e-4b7c-8c6c-95775986fa10\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72b8fec8-298e-4b7c-8c6c-95775986fa10')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-72b8fec8-298e-4b7c-8c6c-95775986fa10 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3587,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3437,\n        \"samples\": [\n          \"B08FRRYWGW\",\n          \"B0020HEBX8\",\n          \"B06WP5W577\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3403,\n        \"samples\": [\n          \"84 PCS Garden Tools Set,10pcs Succulent Tools Set Included 6pcs Large Heavy Duty Aluminum Gardening Hand Tools 12.5IN with Garden Tool Bag,Gloves Sprayer etc.Gardening Gifts for Men Women Garden Gifts\",\n          \"Diaper Dekor Eko Classic Refills, 4 Count | ONLY Eco-Friendly Diaper Pail Refills Made With 70% Recycled Materials | Quicker, Easier & More Cost Effective than Trash Bags | Unscented\",\n          \"Virgin Forest Bamboo Toothbrush, Biodegradable Bamboo Charcoal Toothbrushes, Natural Eco Friendly Wooden Toothbrush 6-Pack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 829,\n        \"samples\": [\n          \"Play Tools\",\n          \"Meals\",\n          \"Machines\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Clothing, Shoes & Jewelry\",\n          \"Video Games\",\n          \"Patio, Lawn & Garden\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"material\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1456,\n        \"samples\": [\n          \"King\",\n          \"cedarwood handle, Charcoal Infused Bristle\",\n          \"Unscented Hardwood\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brand\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2044,\n        \"samples\": [\n          \"Joyborn\",\n          \"Alwayspon\",\n          \"SOTTOG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 794,\n        \"samples\": [\n          \"$357.85\",\n          \"$142.0\",\n          \"$74.95\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33332965524565006,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          5.0,\n          3.6,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviewsCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13350.874821377141,\n        \"min\": 1.0,\n        \"max\": 267025.0,\n        \"num_unique_values\": 1646,\n        \"samples\": [\n          7578.0,\n          4638.0,\n          13156.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2964,\n        \"samples\": [\n          \"100% spun-polyester fabric > Double-sided print > Filled with 100% polyester and sewn closed > Individually cut and sewn by hand > Spot clean/dry clean only\",\n          \"ALL-DAY HYDRATION & FIRMING SUPPORT: The Deeper Dive Moisturizer Serum offers deep hydration with natural post-biotic bacillus ferment, brown algae collagen, and vegan squalane, while the Peak Performance SPF 50 delivers long-lasting moisture and hydration, keeping skin firm, healthy, and glowing. > BROAD-SPECTRUM SUN PROTECTION: The Peak Performance SPF 50 provides powerful protection against UVA/UVB rays and blue light with its non-nano zinc formula. It offers 80 minutes of water resistance, perfect for beach days, outdoor adventures, and active lifestyles. > MICROBIOME SUPPORT & NOURISHMENT: Both products enhance your skin's microbiome with ingredients like vegan squalane and B-Silk Vegan Spider Silk Protein to nourish and support a healthy skin microbiome, ensuring your skin stays hydrated, balanced, and resilient. > ECO-FRIENDLY & REEF-SAFE FORMULA: Both products are free from microplastics and comply with Hawaii's Reef Compliant Act 104, ensuring they\\u2019re safe for marine life. The eco-friendly packaging is made from 100% recycled, ocean-bound plastic, promoting sustainability. > DERMATOLOGIST TESTED & CRUELTY-FREE: Both products are dermatologist-tested, fragrance-free, and suitable for all skin types, including sensitive skin. Co-founded by Kelly Slater, these skincare essentials combine effective protection with eco-conscious values, ensuring the highest standards of quality and safety.\",\n          \"Disposable Plates: Ibambo's 7 inch disposable plates come in a pack of 50 and are perfect for any event. These PFAS-free, compostable plates are a great alternative to traditional plastic or paper plates because they reduce waste > Microwave Safe: Our biodegradable plates are microwaveable and freezer safe. They are also water, heat and oil-resistant, handling a wide range of temperatures and liquids without the plates warping or leaking > Durable and Sturdy: The disposable bamboo paper plates made from 30% bamboo and 70% bagasse are perfect as appetizer plates or dessert plates. The heavy duty plates sturdy construction ensures they won't bend or collapse under the weight of your food > Versatile Use: The bamboo disposable plates elevate any table setting. These 7 inch round plates are perfect for weddings, parties, picnics, camping, and more. These heavy duty plates are ideal for appetizers, desserts, and salads > Compostable and Biodegradable: These disposable plates are designed with the planet in mind. Perfect for individuals and businesses seeking sustainable solutions. Ensure guilt free disposal, as they are home compostable, breaking down into organic matter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3437,\n        \"samples\": [\n          \"https://www.amazon.com/dp/B08FRRYWGW\",\n          \"https://www.amazon.com/dp/B0020HEBX8\",\n          \"https://www.amazon.com/dp/B06WP5W577\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"img_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3415,\n        \"samples\": [\n          \"https://m.media-amazon.com/images/I/81XrrFYFBsL.__AC_SY300_SX300_QL70_ML2_.jpg\",\n          \"https://m.media-amazon.com/images/I/41Mee0tQIFL._SX300_SY300_QL70_ML2_.jpg\",\n          \"https://m.media-amazon.com/images/I/71ZE1vr3LNL.__AC_SY300_SX300_QL70_ML2_.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inStock\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inStockText\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 107,\n        \"samples\": [\n          \"In Stock  Only 10 left in stock - order soon.\",\n          \"Only 18 left in stock - order soon.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = (\n",
        "    df[\"title\"].astype(str) + \" \" +\n",
        "    df[\"material\"].astype(str) + \" \" +\n",
        "    df[\"description\"].astype(str)\n",
        ")"
      ],
      "metadata": {
        "id": "6Q7wRl6c-NPH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eco_terms = [\"eco\", \"recycl\", \"sustain\", \"biodegrad\", \"organic\", \"green\"]\n",
        "df[\"label\"] = df[\"text\"].str.contains(\"|\".join(eco_terms), case=False, regex=True).astype(int)\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df[\"label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDWJ04RI-NYX",
        "outputId": "66435f8f-d8e5-4c7d-8602-4d09fcf56211"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            "label\n",
            "1    2841\n",
            "0     746\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
        "    df[\"text\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"], random_state=SEED\n",
        ")\n",
        "print(f\"Train size: {len(train_texts)}, Eval size: {len(eval_texts)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw6rbSK5-Njf",
        "outputId": "868d5be7-743f-48ee-8c00-31bce5ab403b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 2869, Eval size: 718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from datasets import Dataset\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_untokenized_ds = Dataset.from_pandas(pd.DataFrame({\"text\": train_texts, \"labels\": train_labels}))\n",
        "eval_untokenized_ds  = Dataset.from_pandas(pd.DataFrame({\"text\": eval_texts,  \"labels\": eval_labels}))\n",
        "\n",
        "tokenized_train = train_untokenized_ds.map(tokenize_function, batched=True)\n",
        "tokenized_eval  = eval_untokenized_ds.map(tokenize_function,  batched=True)\n",
        "\n",
        "train_ds = tokenized_train\n",
        "val_ds = tokenized_eval\n",
        "num_labels = df[\"label\"].nunique()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fb971367a0de4474bb3e909fe978acb1",
            "71ae27e6ffbc4de781744e052aade6dc",
            "9b647755616d40dc8c2c185cc72cf942",
            "01103a090116433ca58eef7499a2dd65",
            "75e7d4bb665e40cebc023a9c22349992",
            "0a466be590304b4eb361e142ba02cde6",
            "848e9b12d1b7448da878b6e7e3dd787b",
            "26ffcf0a24174eba81759bc224fc6ee6",
            "8ef9f031fd7c44af903d4854bc00a170",
            "515646b583104163a5ff90edd27d4ea7",
            "b0aa5ac6da524b0f8817fb55d715db52",
            "dbcbb09a91684c8a8d18d07f4ee3839e",
            "9f98a1644b6d4e14a018c1a260fe0d9b",
            "339062c31a5549b18358f0f85972cf72",
            "1fa5de09afab4549b7a30ee572814dd9",
            "a221e2ba28844afea53663dd148e21c2",
            "706228efe6a54295ad761774b855ea2c",
            "1b8e7bd936174689af806ce99868f0a8",
            "3463a5183c9d4f5d8e84c2cae7cf3c8f",
            "b25bee5320e94221891b887caafe47cc",
            "d706dcdce24d46c6a8ac70def6d31d0a",
            "c0c6822aa0ff4ea38962765be9d69660"
          ]
        },
        "id": "J5NzRm8C-Wg-",
        "outputId": "368e34d2-e58f-481e-a7a0-4c7de875aebd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2869 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb971367a0de4474bb3e909fe978acb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbcbb09a91684c8a8d18d07f4ee3839e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    f1 = f1_score(p.label_ids, preds, average=\"binary\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ],
      "metadata": {
        "id": "omiMtRpS-fYI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grid Search Block**"
      ],
      "metadata": {
        "id": "HG_AIsmGydfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🧩 GRID SEARCH: Exhaustive combination testing\n",
        "# =====================================================\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # ✅ add this line to skip W&B login\n",
        "import os, time, itertools, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from google.colab import files\n",
        "\n",
        "# ---------- Setup ----------\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LOG_PATH = \"/content/GridSearch_Experiment_Log.xlsx\"\n",
        "\n",
        "# Dummy check - ensure train_ds, val_ds, num_labels exist before running\n",
        "assert \"train_ds\" in globals() and \"val_ds\" in globals() and \"num_labels\" in globals(), \\\n",
        "    \"Please define train_ds, val_ds, and num_labels first.\"\n",
        "\n",
        "# ---------- Parameter Grid ----------\n",
        "grid_learning_rate = [2e-5, 3e-5, 5e-5]\n",
        "grid_batch_size    = [8, 16, 32]\n",
        "grid_epochs        = [2, 3, 4]\n",
        "\n",
        "# Create Excel header if not exists\n",
        "if not os.path.exists(LOG_PATH):\n",
        "    pd.DataFrame(columns=[\n",
        "        \"Run_ID\",\"learning_rate\",\"batch_size\",\"epochs\",\n",
        "        \"Accuracy\",\"F1_weighted\",\"Train_Time_s\"\n",
        "    ]).to_excel(LOG_PATH, index=False)\n",
        "\n",
        "# ---------- Metric Function ----------\n",
        "def compute_metrics(eval_pred):\n",
        "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
        "    acc = accuracy_score(eval_pred.label_ids, preds)\n",
        "    f1 = f1_score(eval_pred.label_ids, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# ---------- Run All Combinations ----------\n",
        "combinations = list(itertools.product(grid_learning_rate, grid_batch_size, grid_epochs))\n",
        "print(f\"🔍 Total combinations: {len(combinations)}\")\n",
        "\n",
        "for run_id, (lr, bs, ep) in enumerate(combinations, start=1):\n",
        "    print(f\"\\n▶️ Run {run_id}: LR={lr}, BS={bs}, EPOCHS={ep}\")\n",
        "    start = time.time()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"/content/run_{run_id}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=bs,\n",
        "        num_train_epochs=ep,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"/content/logs_{run_id}\",\n",
        "        disable_tqdm=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    elapsed = round(time.time() - start, 2)\n",
        "\n",
        "    row = pd.DataFrame([{\n",
        "        \"Run_ID\": run_id,\n",
        "        \"learning_rate\": lr,\n",
        "        \"batch_size\": bs,\n",
        "        \"epochs\": ep,\n",
        "        \"Accuracy\": metrics[\"eval_accuracy\"],\n",
        "        \"F1_weighted\": metrics[\"eval_f1\"],\n",
        "        \"Train_Time_s\": elapsed\n",
        "    }])\n",
        "    df = pd.read_excel(LOG_PATH)\n",
        "    df = pd.concat([df, row], ignore_index=True)\n",
        "    df.to_excel(LOG_PATH, index=False)\n",
        "\n",
        "files.download(LOG_PATH)\n",
        "print(\"✅ Grid Search completed and logged.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PmGM7bZOyhDa",
        "outputId": "77329424-7a78-4a58-ab23-25d5fc40bcca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Total combinations: 27\n",
            "\n",
            "▶️ Run 1: LR=2e-05, BS=8, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.22539229691028595, 'eval_accuracy': 0.9052924791086351, 'eval_f1': 0.8965555310180436, 'eval_runtime': 2.5714, 'eval_samples_per_second': 279.223, 'eval_steps_per_second': 35.0, 'epoch': 1.0}\n",
            "{'loss': 0.2564, 'grad_norm': 0.046344418078660965, 'learning_rate': 6.100278551532034e-06, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.20089547336101532, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9205293483104027, 'eval_runtime': 2.6258, 'eval_samples_per_second': 273.439, 'eval_steps_per_second': 34.275, 'epoch': 2.0}\n",
            "{'train_runtime': 103.488, 'train_samples_per_second': 55.446, 'train_steps_per_second': 6.938, 'train_loss': 0.23282885219393334, 'epoch': 2.0}\n",
            "{'eval_loss': 0.20089547336101532, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9205293483104027, 'eval_runtime': 2.5405, 'eval_samples_per_second': 282.621, 'eval_steps_per_second': 35.426, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 2: LR=2e-05, BS=8, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3776160938.py:84: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, row], ignore_index=True)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.28230613470077515, 'eval_accuracy': 0.8788300835654597, 'eval_f1': 0.8609080307753963, 'eval_runtime': 2.5528, 'eval_samples_per_second': 281.261, 'eval_steps_per_second': 35.256, 'epoch': 1.0}\n",
            "{'loss': 0.2726, 'grad_norm': 0.05804693326354027, 'learning_rate': 1.0733519034354689e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.19463302195072174, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9205293483104027, 'eval_runtime': 2.5833, 'eval_samples_per_second': 277.943, 'eval_steps_per_second': 34.84, 'epoch': 2.0}\n",
            "{'loss': 0.1609, 'grad_norm': 5.977817058563232, 'learning_rate': 1.4484679665738164e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.24896369874477386, 'eval_accuracy': 0.9206128133704735, 'eval_f1': 0.917578038477194, 'eval_runtime': 2.5759, 'eval_samples_per_second': 278.734, 'eval_steps_per_second': 34.939, 'epoch': 3.0}\n",
            "{'train_runtime': 142.255, 'train_samples_per_second': 60.504, 'train_steps_per_second': 7.571, 'train_loss': 0.2097588169873591, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24896369874477386, 'eval_accuracy': 0.9206128133704735, 'eval_f1': 0.917578038477194, 'eval_runtime': 2.5469, 'eval_samples_per_second': 281.913, 'eval_steps_per_second': 35.337, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 3: LR=2e-05, BS=8, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2059139609336853, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.9125508220948773, 'eval_runtime': 2.5744, 'eval_samples_per_second': 278.904, 'eval_steps_per_second': 34.96, 'epoch': 1.0}\n",
            "{'loss': 0.254, 'grad_norm': 0.03891076520085335, 'learning_rate': 1.3050139275766019e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.16040506958961487, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.933311624622386, 'eval_runtime': 2.5537, 'eval_samples_per_second': 281.164, 'eval_steps_per_second': 35.243, 'epoch': 2.0}\n",
            "{'loss': 0.1522, 'grad_norm': 18.2607421875, 'learning_rate': 6.0863509749303625e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.23612117767333984, 'eval_accuracy': 0.9289693593314763, 'eval_f1': 0.9290569036773025, 'eval_runtime': 2.5141, 'eval_samples_per_second': 285.586, 'eval_steps_per_second': 35.798, 'epoch': 3.0}\n",
            "{'eval_loss': 0.26254382729530334, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9305838050690375, 'eval_runtime': 2.5432, 'eval_samples_per_second': 282.326, 'eval_steps_per_second': 35.389, 'epoch': 4.0}\n",
            "{'train_runtime': 183.1089, 'train_samples_per_second': 62.673, 'train_steps_per_second': 7.842, 'train_loss': 0.16358972060647184, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.26254382729530334, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9305838050690375, 'eval_runtime': 2.5046, 'eval_samples_per_second': 286.67, 'eval_steps_per_second': 35.934, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 4: LR=2e-05, BS=16, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.26055020093917847, 'eval_accuracy': 0.8802228412256268, 'eval_f1': 0.8628311547893537, 'eval_runtime': 2.5695, 'eval_samples_per_second': 279.436, 'eval_steps_per_second': 35.027, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1673114150762558, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9277542600075849, 'eval_runtime': 2.5664, 'eval_samples_per_second': 279.77, 'eval_steps_per_second': 35.069, 'epoch': 2.0}\n",
            "{'train_runtime': 78.6282, 'train_samples_per_second': 72.976, 'train_steps_per_second': 4.579, 'train_loss': 0.2461229748196072, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1673114150762558, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9277542600075849, 'eval_runtime': 2.4826, 'eval_samples_per_second': 289.212, 'eval_steps_per_second': 36.252, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 5: LR=2e-05, BS=16, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.29843196272850037, 'eval_accuracy': 0.8537604456824512, 'eval_f1': 0.8222331718135472, 'eval_runtime': 2.6124, 'eval_samples_per_second': 274.845, 'eval_steps_per_second': 34.451, 'epoch': 1.0}\n",
            "{'eval_loss': 0.17719878256320953, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9270224471601965, 'eval_runtime': 2.5059, 'eval_samples_per_second': 286.522, 'eval_steps_per_second': 35.915, 'epoch': 2.0}\n",
            "{'loss': 0.2192, 'grad_norm': 7.866615295410156, 'learning_rate': 1.5185185185185186e-06, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.19248273968696594, 'eval_accuracy': 0.9206128133704735, 'eval_f1': 0.9185615049139941, 'eval_runtime': 2.5916, 'eval_samples_per_second': 277.046, 'eval_steps_per_second': 34.727, 'epoch': 3.0}\n",
            "{'train_runtime': 124.6904, 'train_samples_per_second': 69.027, 'train_steps_per_second': 4.331, 'train_loss': 0.21190947250083642, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19248273968696594, 'eval_accuracy': 0.9206128133704735, 'eval_f1': 0.9185615049139941, 'eval_runtime': 2.5171, 'eval_samples_per_second': 285.247, 'eval_steps_per_second': 35.755, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 6: LR=2e-05, BS=16, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19956116378307343, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.9107519510798534, 'eval_runtime': 2.5609, 'eval_samples_per_second': 280.37, 'eval_steps_per_second': 35.144, 'epoch': 1.0}\n",
            "{'eval_loss': 0.16161207854747772, 'eval_accuracy': 0.9373259052924791, 'eval_f1': 0.9369282258391112, 'eval_runtime': 2.5065, 'eval_samples_per_second': 286.454, 'eval_steps_per_second': 35.906, 'epoch': 2.0}\n",
            "{'loss': 0.1964, 'grad_norm': 8.274953842163086, 'learning_rate': 6.13888888888889e-06, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.18855419754981995, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9282669467487902, 'eval_runtime': 2.5485, 'eval_samples_per_second': 281.734, 'eval_steps_per_second': 35.315, 'epoch': 3.0}\n",
            "{'eval_loss': 0.2170991599559784, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9307737150333095, 'eval_runtime': 2.549, 'eval_samples_per_second': 281.682, 'eval_steps_per_second': 35.308, 'epoch': 4.0}\n",
            "{'train_runtime': 156.3222, 'train_samples_per_second': 73.412, 'train_steps_per_second': 4.606, 'train_loss': 0.15887888272603354, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2170991599559784, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9307737150333095, 'eval_runtime': 2.5531, 'eval_samples_per_second': 281.227, 'eval_steps_per_second': 35.251, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 7: LR=2e-05, BS=32, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2523709237575531, 'eval_accuracy': 0.8955431754874652, 'eval_f1': 0.8943264983141475, 'eval_runtime': 2.5713, 'eval_samples_per_second': 279.234, 'eval_steps_per_second': 35.001, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1988731175661087, 'eval_accuracy': 0.9178272980501393, 'eval_f1': 0.9173058961001679, 'eval_runtime': 2.5859, 'eval_samples_per_second': 277.658, 'eval_steps_per_second': 34.804, 'epoch': 2.0}\n",
            "{'train_runtime': 66.3601, 'train_samples_per_second': 86.468, 'train_steps_per_second': 2.712, 'train_loss': 0.3111463122897678, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1988731175661087, 'eval_accuracy': 0.9178272980501393, 'eval_f1': 0.9173058961001679, 'eval_runtime': 2.5095, 'eval_samples_per_second': 286.116, 'eval_steps_per_second': 35.864, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 8: LR=2e-05, BS=32, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24815095961093903, 'eval_accuracy': 0.8899721448467967, 'eval_f1': 0.8831087260349344, 'eval_runtime': 2.5549, 'eval_samples_per_second': 281.031, 'eval_steps_per_second': 35.227, 'epoch': 1.0}\n",
            "{'eval_loss': 0.20969635248184204, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9146220240828278, 'eval_runtime': 2.5323, 'eval_samples_per_second': 283.542, 'eval_steps_per_second': 35.542, 'epoch': 2.0}\n",
            "{'eval_loss': 0.19122298061847687, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.9242764870252929, 'eval_runtime': 2.5707, 'eval_samples_per_second': 279.307, 'eval_steps_per_second': 35.011, 'epoch': 3.0}\n",
            "{'train_runtime': 101.8338, 'train_samples_per_second': 84.52, 'train_steps_per_second': 2.651, 'train_loss': 0.2505610430682147, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19122298061847687, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.9242764870252929, 'eval_runtime': 2.5004, 'eval_samples_per_second': 287.159, 'eval_steps_per_second': 35.995, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 9: LR=2e-05, BS=32, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.20999333262443542, 'eval_accuracy': 0.9136490250696379, 'eval_f1': 0.9096358494980398, 'eval_runtime': 2.5774, 'eval_samples_per_second': 278.577, 'eval_steps_per_second': 34.919, 'epoch': 1.0}\n",
            "{'eval_loss': 0.18423132598400116, 'eval_accuracy': 0.9122562674094707, 'eval_f1': 0.909725158780655, 'eval_runtime': 2.574, 'eval_samples_per_second': 278.949, 'eval_steps_per_second': 34.966, 'epoch': 2.0}\n",
            "{'eval_loss': 0.18060967326164246, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.9223845341156736, 'eval_runtime': 2.5355, 'eval_samples_per_second': 283.183, 'eval_steps_per_second': 35.496, 'epoch': 3.0}\n",
            "{'eval_loss': 0.20291048288345337, 'eval_accuracy': 0.9122562674094707, 'eval_f1': 0.9083259414631348, 'eval_runtime': 2.5985, 'eval_samples_per_second': 276.315, 'eval_steps_per_second': 34.636, 'epoch': 4.0}\n",
            "{'train_runtime': 133.8851, 'train_samples_per_second': 85.715, 'train_steps_per_second': 2.689, 'train_loss': 0.19279333750406902, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.20291048288345337, 'eval_accuracy': 0.9122562674094707, 'eval_f1': 0.9083259414631348, 'eval_runtime': 2.5189, 'eval_samples_per_second': 285.045, 'eval_steps_per_second': 35.73, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 10: LR=3e-05, BS=8, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2044772058725357, 'eval_accuracy': 0.9025069637883009, 'eval_f1': 0.8935130466362214, 'eval_runtime': 2.5461, 'eval_samples_per_second': 281.995, 'eval_steps_per_second': 35.348, 'epoch': 1.0}\n",
            "{'loss': 0.2428, 'grad_norm': 0.039347924292087555, 'learning_rate': 9.15041782729805e-06, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.18025580048561096, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.9242158333926502, 'eval_runtime': 2.5694, 'eval_samples_per_second': 279.441, 'eval_steps_per_second': 35.027, 'epoch': 2.0}\n",
            "{'train_runtime': 94.7375, 'train_samples_per_second': 60.567, 'train_steps_per_second': 7.579, 'train_loss': 0.22074160668843304, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.18025580048561096, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.9242158333926502, 'eval_runtime': 2.5384, 'eval_samples_per_second': 282.85, 'eval_steps_per_second': 35.455, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 11: LR=3e-05, BS=8, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21997268497943878, 'eval_accuracy': 0.915041782729805, 'eval_f1': 0.9094252957127131, 'eval_runtime': 2.5482, 'eval_samples_per_second': 281.766, 'eval_steps_per_second': 35.319, 'epoch': 1.0}\n",
            "{'loss': 0.2584, 'grad_norm': 0.027821406722068787, 'learning_rate': 1.610027855153203e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.18805450201034546, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.9346210680947691, 'eval_runtime': 2.5475, 'eval_samples_per_second': 281.846, 'eval_steps_per_second': 35.329, 'epoch': 2.0}\n",
            "{'loss': 0.1297, 'grad_norm': 3.836090326309204, 'learning_rate': 2.1727019498607245e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.2582504451274872, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9272107615663333, 'eval_runtime': 2.5548, 'eval_samples_per_second': 281.035, 'eval_steps_per_second': 35.227, 'epoch': 3.0}\n",
            "{'train_runtime': 155.8816, 'train_samples_per_second': 55.215, 'train_steps_per_second': 6.909, 'train_loss': 0.18760286665892534, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2582504451274872, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9272107615663333, 'eval_runtime': 2.5301, 'eval_samples_per_second': 283.781, 'eval_steps_per_second': 35.571, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 12: LR=3e-05, BS=8, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1957150548696518, 'eval_accuracy': 0.871866295264624, 'eval_f1': 0.84884051441649, 'eval_runtime': 2.5634, 'eval_samples_per_second': 280.102, 'eval_steps_per_second': 35.11, 'epoch': 1.0}\n",
            "{'loss': 0.2383, 'grad_norm': 0.024651166051626205, 'learning_rate': 1.9575208913649026e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.1796017289161682, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.9221968953927837, 'eval_runtime': 2.5749, 'eval_samples_per_second': 278.845, 'eval_steps_per_second': 34.953, 'epoch': 2.0}\n",
            "{'loss': 0.1355, 'grad_norm': 2.2541422843933105, 'learning_rate': 9.129526462395544e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.30048003792762756, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.9160124171919228, 'eval_runtime': 2.5403, 'eval_samples_per_second': 282.64, 'eval_steps_per_second': 35.428, 'epoch': 3.0}\n",
            "{'eval_loss': 0.33089059591293335, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.925521659733915, 'eval_runtime': 2.5566, 'eval_samples_per_second': 280.846, 'eval_steps_per_second': 35.204, 'epoch': 4.0}\n",
            "{'train_runtime': 254.8246, 'train_samples_per_second': 45.035, 'train_steps_per_second': 5.635, 'train_loss': 0.14624426823140518, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.33089059591293335, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.925521659733915, 'eval_runtime': 2.5136, 'eval_samples_per_second': 285.649, 'eval_steps_per_second': 35.806, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 13: LR=3e-05, BS=16, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1990806609392166, 'eval_accuracy': 0.9122562674094707, 'eval_f1': 0.904717935250551, 'eval_runtime': 2.613, 'eval_samples_per_second': 274.776, 'eval_steps_per_second': 34.443, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1564238965511322, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.9369715521082504, 'eval_runtime': 2.6072, 'eval_samples_per_second': 275.386, 'eval_steps_per_second': 34.519, 'epoch': 2.0}\n",
            "{'train_runtime': 138.7667, 'train_samples_per_second': 41.35, 'train_steps_per_second': 2.594, 'train_loss': 0.2222433090209961, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1564238965511322, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.9369715521082504, 'eval_runtime': 2.5259, 'eval_samples_per_second': 284.25, 'eval_steps_per_second': 35.63, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 14: LR=3e-05, BS=16, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.22395634651184082, 'eval_accuracy': 0.9025069637883009, 'eval_f1': 0.8947306578832324, 'eval_runtime': 2.6669, 'eval_samples_per_second': 269.224, 'eval_steps_per_second': 33.747, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1843186914920807, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.9231095320037168, 'eval_runtime': 2.5469, 'eval_samples_per_second': 281.907, 'eval_steps_per_second': 35.337, 'epoch': 2.0}\n",
            "{'loss': 0.2005, 'grad_norm': 8.743749618530273, 'learning_rate': 2.2777777777777776e-06, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.21236340701580048, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9231788790989389, 'eval_runtime': 2.5757, 'eval_samples_per_second': 278.755, 'eval_steps_per_second': 34.941, 'epoch': 3.0}\n",
            "{'train_runtime': 170.4351, 'train_samples_per_second': 50.5, 'train_steps_per_second': 3.168, 'train_loss': 0.1928958186396846, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21236340701580048, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9231788790989389, 'eval_runtime': 2.5373, 'eval_samples_per_second': 282.976, 'eval_steps_per_second': 35.471, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 15: LR=3e-05, BS=16, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.22669248282909393, 'eval_accuracy': 0.8690807799442897, 'eval_f1': 0.8463486552904775, 'eval_runtime': 2.5909, 'eval_samples_per_second': 277.126, 'eval_steps_per_second': 34.737, 'epoch': 1.0}\n",
            "{'eval_loss': 0.16861726343631744, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9331476323119777, 'eval_runtime': 2.5149, 'eval_samples_per_second': 285.501, 'eval_steps_per_second': 35.787, 'epoch': 2.0}\n",
            "{'loss': 0.1884, 'grad_norm': 4.744013786315918, 'learning_rate': 9.208333333333335e-06, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.2154066264629364, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.933936566604142, 'eval_runtime': 2.5268, 'eval_samples_per_second': 284.159, 'eval_steps_per_second': 35.619, 'epoch': 3.0}\n",
            "{'eval_loss': 0.26048076152801514, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.9346210680947691, 'eval_runtime': 2.5387, 'eval_samples_per_second': 282.818, 'eval_steps_per_second': 35.451, 'epoch': 4.0}\n",
            "{'train_runtime': 205.1692, 'train_samples_per_second': 55.934, 'train_steps_per_second': 3.509, 'train_loss': 0.14791381888919405, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.26048076152801514, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.9346210680947691, 'eval_runtime': 2.4756, 'eval_samples_per_second': 290.025, 'eval_steps_per_second': 36.354, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 16: LR=3e-05, BS=32, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.18981704115867615, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9316699422201733, 'eval_runtime': 2.6096, 'eval_samples_per_second': 275.139, 'eval_steps_per_second': 34.488, 'epoch': 1.0}\n",
            "{'eval_loss': 0.164738267660141, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9318389858860358, 'eval_runtime': 2.5188, 'eval_samples_per_second': 285.062, 'eval_steps_per_second': 35.732, 'epoch': 2.0}\n",
            "{'train_runtime': 94.4985, 'train_samples_per_second': 60.721, 'train_steps_per_second': 1.905, 'train_loss': 0.26433113945855036, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.164738267660141, 'eval_accuracy': 0.9317548746518106, 'eval_f1': 0.9318389858860358, 'eval_runtime': 2.5211, 'eval_samples_per_second': 284.792, 'eval_steps_per_second': 35.698, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 17: LR=3e-05, BS=32, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21318025887012482, 'eval_accuracy': 0.9025069637883009, 'eval_f1': 0.8966045938988045, 'eval_runtime': 2.5948, 'eval_samples_per_second': 276.706, 'eval_steps_per_second': 34.685, 'epoch': 1.0}\n",
            "{'eval_loss': 0.19113248586654663, 'eval_accuracy': 0.9094707520891365, 'eval_f1': 0.9051096295941894, 'eval_runtime': 2.5325, 'eval_samples_per_second': 283.515, 'eval_steps_per_second': 35.538, 'epoch': 2.0}\n",
            "{'eval_loss': 0.18659165501594543, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.9259055490217635, 'eval_runtime': 2.5433, 'eval_samples_per_second': 282.307, 'eval_steps_per_second': 35.387, 'epoch': 3.0}\n",
            "{'train_runtime': 133.8499, 'train_samples_per_second': 64.303, 'train_steps_per_second': 2.017, 'train_loss': 0.2121474230730975, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.18659165501594543, 'eval_accuracy': 0.9261838440111421, 'eval_f1': 0.9259055490217635, 'eval_runtime': 2.4959, 'eval_samples_per_second': 287.67, 'eval_steps_per_second': 36.059, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 18: LR=3e-05, BS=32, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19163952767848969, 'eval_accuracy': 0.9122562674094707, 'eval_f1': 0.9112342585838838, 'eval_runtime': 2.6334, 'eval_samples_per_second': 272.654, 'eval_steps_per_second': 34.177, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1649215817451477, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9242156182048196, 'eval_runtime': 2.5828, 'eval_samples_per_second': 277.99, 'eval_steps_per_second': 34.845, 'epoch': 2.0}\n",
            "{'eval_loss': 0.17105722427368164, 'eval_accuracy': 0.9289693593314763, 'eval_f1': 0.9301969905397053, 'eval_runtime': 2.5235, 'eval_samples_per_second': 284.527, 'eval_steps_per_second': 35.665, 'epoch': 3.0}\n",
            "{'eval_loss': 0.2025417685508728, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.920470037127117, 'eval_runtime': 2.8263, 'eval_samples_per_second': 254.044, 'eval_steps_per_second': 31.844, 'epoch': 4.0}\n",
            "{'train_runtime': 176.6418, 'train_samples_per_second': 64.968, 'train_steps_per_second': 2.038, 'train_loss': 0.16719703674316405, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2025417685508728, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.920470037127117, 'eval_runtime': 2.5003, 'eval_samples_per_second': 287.163, 'eval_steps_per_second': 35.995, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 19: LR=5e-05, BS=8, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.18767477571964264, 'eval_accuracy': 0.8913649025069638, 'eval_f1': 0.878368482162278, 'eval_runtime': 2.5917, 'eval_samples_per_second': 277.037, 'eval_steps_per_second': 34.726, 'epoch': 1.0}\n",
            "{'loss': 0.2461, 'grad_norm': 0.030625449493527412, 'learning_rate': 1.5250696378830085e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.1765427440404892, 'eval_accuracy': 0.9303621169916435, 'eval_f1': 0.9310259103353753, 'eval_runtime': 2.5811, 'eval_samples_per_second': 278.177, 'eval_steps_per_second': 34.869, 'epoch': 2.0}\n",
            "{'train_runtime': 149.0977, 'train_samples_per_second': 38.485, 'train_steps_per_second': 4.816, 'train_loss': 0.2239378926481709, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1765427440404892, 'eval_accuracy': 0.9303621169916435, 'eval_f1': 0.9310259103353753, 'eval_runtime': 2.5575, 'eval_samples_per_second': 280.742, 'eval_steps_per_second': 35.19, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 20: LR=5e-05, BS=8, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2552918791770935, 'eval_accuracy': 0.883008356545961, 'eval_f1': 0.8666447718225262, 'eval_runtime': 2.5451, 'eval_samples_per_second': 282.115, 'eval_steps_per_second': 35.363, 'epoch': 1.0}\n",
            "{'loss': 0.2598, 'grad_norm': 0.03196288272738457, 'learning_rate': 2.6833797585886722e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.14757391810417175, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9342311848086093, 'eval_runtime': 2.565, 'eval_samples_per_second': 279.924, 'eval_steps_per_second': 35.088, 'epoch': 2.0}\n",
            "{'loss': 0.1357, 'grad_norm': 6.736411094665527, 'learning_rate': 3.621169916434541e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.2570977210998535, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9249755777001843, 'eval_runtime': 2.5436, 'eval_samples_per_second': 282.281, 'eval_steps_per_second': 35.383, 'epoch': 3.0}\n",
            "{'train_runtime': 218.9009, 'train_samples_per_second': 39.319, 'train_steps_per_second': 4.92, 'train_loss': 0.1903687931368942, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2570977210998535, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9249755777001843, 'eval_runtime': 2.4925, 'eval_samples_per_second': 288.059, 'eval_steps_per_second': 36.108, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 21: LR=5e-05, BS=8, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.26002633571624756, 'eval_accuracy': 0.8844011142061281, 'eval_f1': 0.8713998017695807, 'eval_runtime': 2.6211, 'eval_samples_per_second': 273.93, 'eval_steps_per_second': 34.337, 'epoch': 1.0}\n",
            "{'loss': 0.2323, 'grad_norm': 0.019132157787680626, 'learning_rate': 3.2625348189415045e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.2201913744211197, 'eval_accuracy': 0.9275766016713092, 'eval_f1': 0.9272107615663333, 'eval_runtime': 2.5784, 'eval_samples_per_second': 278.467, 'eval_steps_per_second': 34.905, 'epoch': 2.0}\n",
            "{'loss': 0.1297, 'grad_norm': 11.113912582397461, 'learning_rate': 1.5215877437325907e-05, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.2526457607746124, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9340853197932145, 'eval_runtime': 2.5033, 'eval_samples_per_second': 286.819, 'eval_steps_per_second': 35.952, 'epoch': 3.0}\n",
            "{'eval_loss': 0.29234832525253296, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.9346210680947691, 'eval_runtime': 2.5233, 'eval_samples_per_second': 284.548, 'eval_steps_per_second': 35.668, 'epoch': 4.0}\n",
            "{'train_runtime': 263.973, 'train_samples_per_second': 43.474, 'train_steps_per_second': 5.44, 'train_loss': 0.1392314480539816, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.29234832525253296, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.9346210680947691, 'eval_runtime': 2.4727, 'eval_samples_per_second': 290.374, 'eval_steps_per_second': 36.398, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 22: LR=5e-05, BS=16, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.17766954004764557, 'eval_accuracy': 0.9080779944289693, 'eval_f1': 0.9058390142883304, 'eval_runtime': 2.6108, 'eval_samples_per_second': 275.015, 'eval_steps_per_second': 34.473, 'epoch': 1.0}\n",
            "{'eval_loss': 0.14901649951934814, 'eval_accuracy': 0.9387186629526463, 'eval_f1': 0.939971578173246, 'eval_runtime': 2.5467, 'eval_samples_per_second': 281.93, 'eval_steps_per_second': 35.339, 'epoch': 2.0}\n",
            "{'train_runtime': 85.4411, 'train_samples_per_second': 67.157, 'train_steps_per_second': 4.213, 'train_loss': 0.21806386311848958, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.14901649951934814, 'eval_accuracy': 0.9387186629526463, 'eval_f1': 0.939971578173246, 'eval_runtime': 2.4556, 'eval_samples_per_second': 292.398, 'eval_steps_per_second': 36.652, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 23: LR=5e-05, BS=16, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24069802463054657, 'eval_accuracy': 0.8732590529247911, 'eval_f1': 0.8538154719384154, 'eval_runtime': 2.6016, 'eval_samples_per_second': 275.987, 'eval_steps_per_second': 34.594, 'epoch': 1.0}\n",
            "{'eval_loss': 0.18198668956756592, 'eval_accuracy': 0.9178272980501393, 'eval_f1': 0.9170901495151128, 'eval_runtime': 2.5286, 'eval_samples_per_second': 283.952, 'eval_steps_per_second': 35.593, 'epoch': 2.0}\n",
            "{'loss': 0.1911, 'grad_norm': 2.078322649002075, 'learning_rate': 3.7962962962962964e-06, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.24599984288215637, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.914643198998821, 'eval_runtime': 2.5697, 'eval_samples_per_second': 279.411, 'eval_steps_per_second': 35.024, 'epoch': 3.0}\n",
            "{'train_runtime': 149.3451, 'train_samples_per_second': 57.632, 'train_steps_per_second': 3.616, 'train_loss': 0.1837824432938187, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24599984288215637, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.914643198998821, 'eval_runtime': 2.5044, 'eval_samples_per_second': 286.691, 'eval_steps_per_second': 35.936, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 24: LR=5e-05, BS=16, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.16799069941043854, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.92274901957562, 'eval_runtime': 2.5883, 'eval_samples_per_second': 277.402, 'eval_steps_per_second': 34.772, 'epoch': 1.0}\n",
            "{'eval_loss': 0.166359081864357, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.9356095198471409, 'eval_runtime': 2.5088, 'eval_samples_per_second': 286.197, 'eval_steps_per_second': 35.874, 'epoch': 2.0}\n",
            "{'loss': 0.1955, 'grad_norm': 1.214633822441101, 'learning_rate': 1.5347222222222226e-05, 'epoch': 2.7777777777777777}\n",
            "{'eval_loss': 0.19971565902233124, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.9365438375085451, 'eval_runtime': 2.5246, 'eval_samples_per_second': 284.407, 'eval_steps_per_second': 35.65, 'epoch': 3.0}\n",
            "{'eval_loss': 0.24574263393878937, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9331476323119777, 'eval_runtime': 2.5788, 'eval_samples_per_second': 278.429, 'eval_steps_per_second': 34.901, 'epoch': 4.0}\n",
            "{'train_runtime': 236.7628, 'train_samples_per_second': 48.47, 'train_steps_per_second': 3.041, 'train_loss': 0.15535171031951905, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24574263393878937, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9331476323119777, 'eval_runtime': 2.5362, 'eval_samples_per_second': 283.106, 'eval_steps_per_second': 35.487, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 25: LR=5e-05, BS=32, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19411712884902954, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.9128305556963533, 'eval_runtime': 2.6144, 'eval_samples_per_second': 274.63, 'eval_steps_per_second': 34.424, 'epoch': 1.0}\n",
            "{'eval_loss': 0.15992268919944763, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.935933147632312, 'eval_runtime': 2.5467, 'eval_samples_per_second': 281.938, 'eval_steps_per_second': 35.34, 'epoch': 2.0}\n",
            "{'train_runtime': 108.3226, 'train_samples_per_second': 52.971, 'train_steps_per_second': 1.662, 'train_loss': 0.22441683875189888, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.15992268919944763, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.935933147632312, 'eval_runtime': 2.4735, 'eval_samples_per_second': 290.272, 'eval_steps_per_second': 36.385, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 26: LR=5e-05, BS=32, EPOCHS=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.21965964138507843, 'eval_accuracy': 0.9080779944289693, 'eval_f1': 0.9058390142883304, 'eval_runtime': 2.6002, 'eval_samples_per_second': 276.132, 'eval_steps_per_second': 34.613, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1788358837366104, 'eval_accuracy': 0.9136490250696379, 'eval_f1': 0.9104863573376397, 'eval_runtime': 2.5969, 'eval_samples_per_second': 276.487, 'eval_steps_per_second': 34.657, 'epoch': 2.0}\n",
            "{'eval_loss': 0.19730593264102936, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.922083862832593, 'eval_runtime': 2.6041, 'eval_samples_per_second': 275.717, 'eval_steps_per_second': 34.561, 'epoch': 3.0}\n",
            "{'train_runtime': 143.7897, 'train_samples_per_second': 59.858, 'train_steps_per_second': 1.878, 'train_loss': 0.19149598015679253, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.19730593264102936, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.922083862832593, 'eval_runtime': 2.5131, 'eval_samples_per_second': 285.702, 'eval_steps_per_second': 35.812, 'epoch': 3.0}\n",
            "\n",
            "▶️ Run 27: LR=5e-05, BS=32, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3776160938.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1819441020488739, 'eval_accuracy': 0.915041782729805, 'eval_f1': 0.9149360505189911, 'eval_runtime': 2.6284, 'eval_samples_per_second': 273.174, 'eval_steps_per_second': 34.242, 'epoch': 1.0}\n",
            "{'eval_loss': 0.1566794514656067, 'eval_accuracy': 0.924791086350975, 'eval_f1': 0.9260100829096853, 'eval_runtime': 2.5215, 'eval_samples_per_second': 284.754, 'eval_steps_per_second': 35.693, 'epoch': 2.0}\n",
            "{'eval_loss': 0.18615204095840454, 'eval_accuracy': 0.9373259052924791, 'eval_f1': 0.9382736970326648, 'eval_runtime': 2.5749, 'eval_samples_per_second': 278.843, 'eval_steps_per_second': 34.952, 'epoch': 3.0}\n",
            "{'eval_loss': 0.22485998272895813, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.920776251724538, 'eval_runtime': 2.5533, 'eval_samples_per_second': 281.202, 'eval_steps_per_second': 35.248, 'epoch': 4.0}\n",
            "{'train_runtime': 155.2008, 'train_samples_per_second': 73.943, 'train_steps_per_second': 2.32, 'train_loss': 0.14499569998847114, 'epoch': 4.0}\n",
            "{'eval_loss': 0.22485998272895813, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.920776251724538, 'eval_runtime': 2.4841, 'eval_samples_per_second': 289.04, 'eval_steps_per_second': 36.231, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19852c92-5ca0-4319-b7ae-744426356586\", \"GridSearch_Experiment_Log.xlsx\", 6318)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Grid Search completed and logged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Search Block**"
      ],
      "metadata": {
        "id": "BBa817x0yklq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🎲 RANDOM SEARCH: Random sampling of combinations\n",
        "# =====================================================\n",
        "import os, time, random, numpy as np, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from google.colab import files\n",
        "\n",
        "# ---------- Setup ----------\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LOG_PATH = \"/content/RandomSearch_Experiment_Log.xlsx\"\n",
        "\n",
        "# Parameter choices\n",
        "grid_learning_rate = [2e-5, 3e-5, 5e-5]\n",
        "grid_batch_size    = [8, 16, 32]\n",
        "grid_epochs        = [2, 3, 4]\n",
        "\n",
        "# Create Excel header if not exists\n",
        "if not os.path.exists(LOG_PATH):\n",
        "    pd.DataFrame(columns=[\n",
        "        \"Run_ID\",\"learning_rate\",\"batch_size\",\"epochs\",\n",
        "        \"Accuracy\",\"F1_weighted\",\"Train_Time_s\"\n",
        "    ]).to_excel(LOG_PATH, index=False)\n",
        "\n",
        "# ---------- Metric Function ----------\n",
        "def compute_metrics(eval_pred):\n",
        "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
        "    acc = accuracy_score(eval_pred.label_ids, preds)\n",
        "    f1 = f1_score(eval_pred.label_ids, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# ---------- Random Sampling ----------\n",
        "num_trials = 5  # <-- Change this to how many random experiments you want\n",
        "random_combos = [\n",
        "    (\n",
        "        random.choice(grid_learning_rate),\n",
        "        random.choice(grid_batch_size),\n",
        "        random.choice(grid_epochs)\n",
        "    )\n",
        "    for _ in range(num_trials)\n",
        "]\n",
        "print(f\"🎯 Randomly selected {num_trials} combinations\")\n",
        "\n",
        "for run_id, (lr, bs, ep) in enumerate(random_combos, start=1):\n",
        "    print(f\"\\n▶️ Run {run_id}: LR={lr}, BS={bs}, EPOCHS={ep}\")\n",
        "    start = time.time()\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"/content/random_run_{run_id}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=bs,\n",
        "        num_train_epochs=ep,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"/content/random_logs_{run_id}\",\n",
        "        disable_tqdm=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "    elapsed = round(time.time() - start, 2)\n",
        "\n",
        "    row = pd.DataFrame([{\n",
        "        \"Run_ID\": run_id,\n",
        "        \"learning_rate\": lr,\n",
        "        \"batch_size\": bs,\n",
        "        \"epochs\": ep,\n",
        "        \"Accuracy\": metrics[\"eval_accuracy\"],\n",
        "        \"F1_weighted\": metrics[\"eval_f1\"],\n",
        "        \"Train_Time_s\": elapsed\n",
        "    }])\n",
        "    df = pd.read_excel(LOG_PATH)\n",
        "    df = pd.concat([df, row], ignore_index=True)\n",
        "    df.to_excel(LOG_PATH, index=False)\n",
        "\n",
        "files.download(LOG_PATH)\n",
        "print(\"✅ Random Search completed and logged.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y7PhJqFByrHu",
        "outputId": "229ba929-f30f-436a-fd85-373d5dd3c77f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Randomly selected 5 combinations\n",
            "\n",
            "▶️ Run 1: LR=5e-05, BS=8, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1495017432.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.18767477571964264, 'eval_accuracy': 0.8913649025069638, 'eval_f1': 0.878368482162278, 'eval_runtime': 3.0642, 'eval_samples_per_second': 234.317, 'eval_steps_per_second': 29.371, 'epoch': 1.0}\n",
            "{'loss': 0.2461, 'grad_norm': 0.030625449493527412, 'learning_rate': 1.5250696378830085e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.1765427440404892, 'eval_accuracy': 0.9303621169916435, 'eval_f1': 0.9310259103353753, 'eval_runtime': 2.8056, 'eval_samples_per_second': 255.917, 'eval_steps_per_second': 32.079, 'epoch': 2.0}\n",
            "{'train_runtime': 146.6898, 'train_samples_per_second': 39.117, 'train_steps_per_second': 4.895, 'train_loss': 0.2239378926481709, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1495017432.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, row], ignore_index=True)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1765427440404892, 'eval_accuracy': 0.9303621169916435, 'eval_f1': 0.9310259103353753, 'eval_runtime': 3.1382, 'eval_samples_per_second': 228.794, 'eval_steps_per_second': 28.679, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 2: LR=5e-05, BS=16, EPOCHS=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1495017432.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2015693187713623, 'eval_accuracy': 0.9164345403899722, 'eval_f1': 0.9143991038984821, 'eval_runtime': 2.5589, 'eval_samples_per_second': 280.587, 'eval_steps_per_second': 35.171, 'epoch': 1.0}\n",
            "{'eval_loss': 0.17779286205768585, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9208716257738241, 'eval_runtime': 2.5607, 'eval_samples_per_second': 280.393, 'eval_steps_per_second': 35.147, 'epoch': 2.0}\n",
            "{'train_runtime': 102.0055, 'train_samples_per_second': 56.252, 'train_steps_per_second': 3.529, 'train_loss': 0.23287635379367405, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.17779286205768585, 'eval_accuracy': 0.9192200557103064, 'eval_f1': 0.9208716257738241, 'eval_runtime': 2.4697, 'eval_samples_per_second': 290.725, 'eval_steps_per_second': 36.442, 'epoch': 2.0}\n",
            "\n",
            "▶️ Run 3: LR=2e-05, BS=8, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1495017432.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.3001568913459778, 'eval_accuracy': 0.8774373259052924, 'eval_f1': 0.8589738086195168, 'eval_runtime': 2.6392, 'eval_samples_per_second': 272.05, 'eval_steps_per_second': 34.101, 'epoch': 1.0}\n",
            "{'loss': 0.2723, 'grad_norm': 0.051783736795186996, 'learning_rate': 1.3050139275766019e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.18903271853923798, 'eval_accuracy': 0.9220055710306406, 'eval_f1': 0.9240720402839743, 'eval_runtime': 2.5478, 'eval_samples_per_second': 281.812, 'eval_steps_per_second': 35.325, 'epoch': 2.0}\n",
            "{'loss': 0.1552, 'grad_norm': 6.58994722366333, 'learning_rate': 6.0863509749303625e-06, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.23396709561347961, 'eval_accuracy': 0.9289693593314763, 'eval_f1': 0.9285186559509926, 'eval_runtime': 2.5097, 'eval_samples_per_second': 286.086, 'eval_steps_per_second': 35.86, 'epoch': 3.0}\n",
            "{'eval_loss': 0.28341197967529297, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.9214189959696434, 'eval_runtime': 2.6225, 'eval_samples_per_second': 273.781, 'eval_steps_per_second': 34.318, 'epoch': 4.0}\n",
            "{'train_runtime': 272.8431, 'train_samples_per_second': 42.061, 'train_steps_per_second': 5.263, 'train_loss': 0.17502560522562946, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.28341197967529297, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.9214189959696434, 'eval_runtime': 2.5291, 'eval_samples_per_second': 283.892, 'eval_steps_per_second': 35.585, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 4: LR=2e-05, BS=32, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1495017432.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.23445384204387665, 'eval_accuracy': 0.8941504178272981, 'eval_f1': 0.8981259457839675, 'eval_runtime': 2.591, 'eval_samples_per_second': 277.111, 'eval_steps_per_second': 34.735, 'epoch': 1.0}\n",
            "{'eval_loss': 0.18130339682102203, 'eval_accuracy': 0.9233983286908078, 'eval_f1': 0.919444080490331, 'eval_runtime': 2.5304, 'eval_samples_per_second': 283.745, 'eval_steps_per_second': 35.567, 'epoch': 2.0}\n",
            "{'eval_loss': 0.15684232115745544, 'eval_accuracy': 0.935933147632312, 'eval_f1': 0.9350999127032655, 'eval_runtime': 2.5601, 'eval_samples_per_second': 280.462, 'eval_steps_per_second': 35.155, 'epoch': 3.0}\n",
            "{'eval_loss': 0.1683124601840973, 'eval_accuracy': 0.9289693593314763, 'eval_f1': 0.9262540344269631, 'eval_runtime': 2.6275, 'eval_samples_per_second': 273.266, 'eval_steps_per_second': 34.253, 'epoch': 4.0}\n",
            "{'train_runtime': 186.1255, 'train_samples_per_second': 61.657, 'train_steps_per_second': 1.934, 'train_loss': 0.20728420681423612, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1683124601840973, 'eval_accuracy': 0.9289693593314763, 'eval_f1': 0.9262540344269631, 'eval_runtime': 2.5033, 'eval_samples_per_second': 286.821, 'eval_steps_per_second': 35.952, 'epoch': 4.0}\n",
            "\n",
            "▶️ Run 5: LR=5e-05, BS=8, EPOCHS=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-1495017432.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.37740209698677063, 'eval_accuracy': 0.8690807799442897, 'eval_f1': 0.8455544386429354, 'eval_runtime': 2.5811, 'eval_samples_per_second': 278.178, 'eval_steps_per_second': 34.869, 'epoch': 1.0}\n",
            "{'loss': 0.2517, 'grad_norm': 0.021372394636273384, 'learning_rate': 3.2625348189415045e-05, 'epoch': 1.392757660167131}\n",
            "{'eval_loss': 0.14069446921348572, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9322781697773205, 'eval_runtime': 2.538, 'eval_samples_per_second': 282.904, 'eval_steps_per_second': 35.461, 'epoch': 2.0}\n",
            "{'loss': 0.1376, 'grad_norm': 0.025354748591780663, 'learning_rate': 1.5215877437325907e-05, 'epoch': 2.785515320334262}\n",
            "{'eval_loss': 0.2600768506526947, 'eval_accuracy': 0.9331476323119777, 'eval_f1': 0.9328099337535383, 'eval_runtime': 2.5163, 'eval_samples_per_second': 285.341, 'eval_steps_per_second': 35.767, 'epoch': 3.0}\n",
            "{'eval_loss': 0.3014090657234192, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.933953169952717, 'eval_runtime': 2.5553, 'eval_samples_per_second': 280.987, 'eval_steps_per_second': 35.221, 'epoch': 4.0}\n",
            "{'train_runtime': 240.4264, 'train_samples_per_second': 47.732, 'train_steps_per_second': 5.973, 'train_loss': 0.1528688792066654, 'epoch': 4.0}\n",
            "{'eval_loss': 0.3014090657234192, 'eval_accuracy': 0.9345403899721448, 'eval_f1': 0.933953169952717, 'eval_runtime': 2.5387, 'eval_samples_per_second': 282.821, 'eval_steps_per_second': 35.451, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fa4dac9a-0831-456a-9388-3f3afb2d41af\", \"RandomSearch_Experiment_Log.xlsx\", 5302)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Random Search completed and logged.\n"
          ]
        }
      ]
    }
  ]
}